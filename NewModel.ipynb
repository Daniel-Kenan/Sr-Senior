{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4321ec97-2b21-4131-9deb-fb6362466107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn==0.0\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting matplotlib==3.10.0\n",
      "  Using cached matplotlib-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy==2.2.3\n",
      "  Using cached numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas==2.2.3\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting scikit-learn==1.6.1\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting seaborn==0.13.2\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting xgboost==2.1.4\n",
      "  Using cached xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting imbalanced-learn (from imblearn==0.0)\n",
      "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.10.0)\n",
      "  Using cached contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.10.0)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.10.0)\n",
      "  Using cached fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib==3.10.0)\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.11/site-packages (from matplotlib==3.10.0) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib==3.10.0)\n",
      "  Using cached pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.10.0)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.conda/lib/python3.11/site-packages (from matplotlib==3.10.0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas==2.2.3)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.3)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.6.1)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.6.1)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.6.1)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost==2.1.4)\n",
      "  Using cached nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib==3.10.0) (1.17.0)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn==0.0)\n",
      "  Using cached sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached matplotlib-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Using cached numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl (223.6 MB)\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/223.6 MB\u001b[0m \u001b[31m254.4 kB/s\u001b[0m eta \u001b[36m0:13:08\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/http/client.py\", line 473, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/ssl.py\", line 1166, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 106, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 97, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/commands/install.py\", line 386, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/sdanielkenan/Desktop/Workspace/JJJ/.conda/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install imblearn==0.0 matplotlib==3.10.0 numpy==2.2.3 pandas==2.2.3 scikit-learn==1.6.1 seaborn==0.13.2 xgboost==2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ff9cb9-b5e4-4c26-863e-e2def97994e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome\n",
      "Improved     74966\n",
      "Unchanged    45095\n",
      "Worsened     29939\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset (replace with your actual file path)\n",
    "data = pd.read_csv('Final_healthcare_big_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_features = ['Daily_Patient_Inflow', 'Emergency_Response_Time_Minutes', 'Staff_Count', \n",
    "                      'Bed_Occupancy_Rate', 'Visit_Frequency', 'Wait_Time_Minutes', \n",
    "                      'Length_of_Stay', 'Previous_Visits', 'Resource_Utilization', 'Age']\n",
    "categorical_features = ['Treatment_Outcome', 'Equipment_Availability', 'Medicine_Stock_Level', \n",
    "                        'Comorbidities', 'Disease_Category']\n",
    "ordinal_features = ['Satisfaction_Rating']  # Treat as numerical/ordinal\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False))  # Updated parameter\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('scaler', StandardScaler())  # Scale ordinal data\n",
    "])\n",
    "\n",
    "# Combine preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('ord', ordinal_transformer, ordinal_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Check for imbalanced target\n",
    "print(y.value_counts())  # Ensure 'Outcome' is balanced or apply balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621443b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f128dd3-f5c0-47ca-879c-31e6c0e5b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Final_healthcare_big_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Encode the target variable (Outcome) to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Converts 'Improved', 'Unchanged', 'Worsened' to 0, 1, 2\n",
    "# Store the mapping for later (optional, for interpretation)\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Encoding Mapping:\", label_mapping)\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_features = ['Daily_Patient_Inflow', 'Emergency_Response_Time_Minutes', 'Staff_Count', \n",
    "                      'Bed_Occupancy_Rate', 'Visit_Frequency', 'Wait_Time_Minutes', \n",
    "                      'Length_of_Stay', 'Previous_Visits', 'Resource_Utilization', 'Age']\n",
    "categorical_features = ['Treatment_Outcome', 'Equipment_Availability', 'Medicine_Stock_Level', \n",
    "                        'Comorbidities', 'Disease_Category']\n",
    "ordinal_features = ['Satisfaction_Rating']  # Treat as numerical/ordinal\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False))  # Updated parameter\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('scaler', StandardScaler())  # Scale ordinal data\n",
    "])\n",
    "\n",
    "# Combine preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('ord', ordinal_transformer, ordinal_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the features\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Handle imbalanced data with SMOTE (on encoded target)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_processed, y_encoded)\n",
    "\n",
    "# Split the balanced data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Model with class weights\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# XGBoost Model with class weights (use numerical labels)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, scale_pos_weight='balanced')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Decode predictions back to original labels for reporting\n",
    "rf_pred_decoded = label_encoder.inverse_transform(rf_pred)\n",
    "xgb_pred_decoded = label_encoder.inverse_transform(xgb_pred)\n",
    "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test_decoded, rf_pred_decoded))\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test_decoded, xgb_pred_decoded))\n",
    "\n",
    "# Confusion Matrix for Random Forest (with decoded labels)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test_decoded, rf_pred_decoded)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Improved', 'Unchanged', 'Worsened'], \n",
    "            yticklabels=['Improved', 'Unchanged', 'Worsened'])\n",
    "plt.title('Confusion Matrix (Random Forest)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance (Random Forest)\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = (numerical_features + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)) + \n",
    "                 ordinal_features)\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1feed-865d-4c7d-96f1-ca607cdd8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy={1: 74966, 2: 74966})  # Target \"Unchanged\" and \"Worsened\" to match \"Improved\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67ff87-6cf0-4b18-92b8-51cfab56de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "oversample = SMOTE(random_state=42)\n",
    "undersample = RandomUnderSampler(random_state=42)\n",
    "pipeline = ImbPipeline(steps=[('over', oversample), ('under', undersample)])\n",
    "X_balanced, y_balanced = pipeline.fit_resample(X_processed, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ca476-2b37-41c9-9348-347cf7f8e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = dict(zip(np.unique(y_encoded), class_weights))\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=class_weight_dict)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, scale_pos_weight=sum(y_encoded==0)/sum(y_encoded==2))  # Adjust for \"Worsened\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00870d-d12f-4b73-b8f6-945813a7af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), param_grid_rf, cv=5, scoring='f1_weighted')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(\"Best RF Params:\", grid_search_rf.best_params_)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3]\n",
    "}\n",
    "grid_search_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, scale_pos_weight='balanced'), param_grid_xgb, cv=5, scoring='f1_weighted')\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "print(\"Best XGBoost Params:\", grid_search_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6051a3-ee79-46dc-838c-c4a00e4b3c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd50f0-13c1-4c18-8f80-453929836534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
